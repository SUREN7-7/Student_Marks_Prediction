END - TO - END MACHINE LEARNING PROJECT

-> DATA COLLECTION/INGESTION

-> DATA PREPROCESSING

-> EXPLORATORY DATA ANALYSIS/DATA VISUALIZATION

-> FEATURE ENGINEERING

-> MODEL SELECTION

-> MODEL TRAINING

-> MODEL EVALUATION

-> HYPER PARAMETER TUNING

-> DEPLOYEMENT
----------------------------------------------------------------------------------------------------------------------------------------

CREATING A VENV AND LINKING IT TO GITHUB REPOSITORY:

* Create a virtual environment(venv) in ML_PROJECT folder where all the required packages are installed.
	###python -m venv venv
	###c.\venv\Scripts\Activate.ps1

{	vs code to github repository	}

//git init
*git add README.md
*git commit -m "first commit"
//git branch -M main
//git remote add origin https://github.com/SUREN7-7/ML_PROJECT.git
*git push -u origin main


{	github repository to vs code	}

*git pull

-----------------------------------------------------------------------------------------------------------------------------------------

creating 2 files setup.py and requirements.py
-> so setup.py is a python program which reads the data from text file ie., required packages and install them
	### pip install -r requirements.txt

The entire project implementation is from src.
creating components as a floder so that it can be used as a package and be utilized.
-> Developing a project initially begins with data ingestion which is a part of a module.
-> data transformation,data_trainer are represented as modules of the package components.

creating a pipeline
-> train_pipeline (which uses the components)
-> predict_pipeline

creating logger,exception,utils python files
logging: Logging is used to track events in a Python application. Instead of using print(), logging provides a better way to debug, monitor, 
	 and store runtime information.
exception: it contains UserDefinedException handling code
utils: It contains the code which can gather data from databases or deploy model in cloud

The sys.exc_info() {exection info}function provides details about the most recent exception that was caught in a try-except block. It returns a tuple with three values:
->Exception Type – The class of the exception (ZeroDivisionError, ValueError, etc.).
->Exception Value – The error message or details of the exception.
->Traceback Object – The traceback, which contains information about where the error occurred.
we actually need 3rd arg which represent the line of error, file

The __init__ method in Python is a special (dunder) method used to initialize an object when it is created. 
The __str__ method in Python is a special (dunder) method used to return a human-readable string representation of an object.

The os module in Python provides functions to interact with the operating system (Windows, Linux, macOS)
-> Work with files & directories
-> Execute system commands
-> Get environment variables
-> Manage processes

logging.debug()		Detailed debugging info					"Variable X has value Y"
logging.info()		General status updates					"Server started"
logging.warning()	Warnings that might cause issues			"Low disk space"
logging.error()		Errors that prevent normal execution			"File not found"
logging.critical()	Serious issues that require immediate attention		"System crash detected!"

check whether logger is working
#python src/logger.py

-----------------------------------------------------------------------------------------------------------------

											DATA INGESTION

This Python script is part of a machine learning pipeline. 
It handles data ingestion and data transformation, with placeholders for model training
DataIngestionConfig (Configuration Class):
	Defines paths where raw, training, and testing data will be stored.
	Uses os.path.join() to construct paths dynamically.
	Stored under the artifacts/ directory.
DataIngestion (Main Class):
	__init__(self)::
		Initializes an instance of DataIngestionConfig to store file paths.
	initiate_data_ingestion():
		This method is responsible for reading the dataset, splitting it into training and testing sets, and saving the files. 
	makedirs extract the folder artifacts and creates it if not exist 
	to_csv() creates a df to csv file and saves in the given path ie 1st arg

----------------------------------------------------------------------------------------------------------------------

											DATA TRANSFORMATION

This module is responsible for preprocessing raw data before it is used for model training. It handles:
	Handling missing values (imputation)
	Scaling numerical features
	Encoding categorical variables
	Saving the preprocessing object (preprocessor.pkl) for later use.
	A PKL file is a file saved in the Python pickle format, which contains serialized Python objects. 
	These files are typically used to store machine learning models, data pre-processing objects, or any Python objects
DataTransformationConfig (Configuration Class):
	Defines a configuration class to store the file path for the saved preprocessing object (proprocessor.pkl).
DataTransformation Class (Main Processing Class):
	__init__(self):
		This ensures that all methods inside this class can access the preprocessor.pkl file path
	get_data_transformer_object():
		This function creates and returns the preprocessing pipeline, which transforms numerical and categorical data.
	initiate_data_transformation(train_path, test_path):
		Reads the train and test CSV files.
		Prepares data for transformation.
		Applies preprocessing.
		Saves the preprocessing object (preprocessor.pkl).
		Returns processed training & testing arrays.
		path is obtained in data_ingestion
		Calls the get_data_transformer_object() function to get the preprocessor pipeline.
	save_object() from utils.py is used to save the path and preprocessor

--------------------------------------------------------------------------------------------------------------------

												MODEL_TRAINER

This function actually decides the best model which has high score and stores the model in "model.pkl" file
ModelTrainerConfig (Configuration Class):
	Defines a configuration class to store the file path for the saved model (model.pkl).
ModelTrainer Class (Main Processing Class):
	__init__(self):
		This ensures that all methods inside this class can access the model.pkl file path
	initiate_model_trainer():
		splits the data to training and testing datasets
		consists of models in form of dictionary
		utils.evaluate_models gives the model report ie the performance of each model
		there by best score and best model can be determined
		finally the func return the best r2_score
	save_object() is used to save path and best model	

---------------------------------------------------------------------------------------------------------------------

												HYPER PARAMETER TUNING
It is nothing but providing parameters in the ml model
It helps in improving the prediction of the model and better accuracy
Hyper parameters can be assigned either by trail and error method 
And also modules are provided such as RandomizedSearchCV or GridSearchCV which are used ot calculate the optimal ones

---------------------------------------------------------------------------------------------------------------------

												PREDICT PIPELINE
CustomData:
	This fn is generally used to convert the data obtained frm web into a dataframe so that it is eligible for
	prediction and this is done by the fn get_data_as_data_frame() 
	This fn finally returns the raw data into df of features
PredictPipeline:
	It contains a predict fn which takes the input from web app
	utils.py is used to load the model and preprocessor
	Hence preprocessing and prediction is done here
	The fn returns predicted value

---------------------------------------------------------------------------------------------------------------------

											HTML PAGE
.This page contains several fields ie features/columns.
.This uses forms
.An HTML form is used to collect user input and send it to a server. 
.It contains form elements like label, text, text fields, radio buttons, checkboxes, dropdowns, and buttons.
.action="{{ url_for('predict_datapoint') }}"
.url_for('predict_datapoint') dynamically generates the URL for the predict_datapoint route in a Flask app.
.This means that when the form is submitted, the data will be sent to the /predict_datapoint route in the Flask backend.
.POST is used when submitting sensitive or large amounts of data, such as user inputs or predictions.
=> <form_element>
=> select is used generate a dropdown 
=>  class="placeholder"
	Adds a CSS class for styling (optional).
	selected
	Makes this option selected by default when the page loads.
	disabled
	Prevents users from selecting this option as a valid input.
	value=""
	The value is empty, so it's not sent when submitting the form.
On submitting the selected values, these values are maped to the fn predict_datapoint() in app.py where 
processing and prediction is done.

---------------------------------------------------------------------------------------------------------------------

												FLASK APP
Flask is a lightweight and flexible Python web framework used to build web applications and APIs. 
It is easy to use and ideal for small to medium-sized projects, including ML model deployment.
from flask import Flask
app = Flask(__name__)
@app.route("/")  # Home route
def home():
    return "Hello, Flask!"	#Here we can either return html pages through render_templates(file.html)
if __name__ == "__main__":
    app.run(debug=True)		#Used to connect from web, site accessed when app.py is running

Intially http://127.0.0.1:5000
http://127.0.0.1:5000/ for home page ie index.html
http://127.0.0.1:5000/predictdata for prediction page
Methods => get for getting data from web
		=> post for printing the data in the API
In this program we connect through html and methods and gather the features 
These features are converted to df and pree=diction is processed in the predict_pipeline
This returns predicted value and it is gained in html
The predicted value is displayed on web page through html by post method.

--------------------------------------------------------------------------------------------------------------------